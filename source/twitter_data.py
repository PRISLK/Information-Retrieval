# -*- coding: utf-8 -*-
"""twitter_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WU0QwuR5x0MppnFv1-Wc5KBeamJ0CoQ3

**Assignment1 - Information Retrieval**

**209381P - T.P.N. Silva**

**Preprocess Twitter Feed**
"""

pip install nltk

pip install Unidecode

# Import libaries

import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk import sent_tokenize, word_tokenize
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords

import pandas as pd
import numpy as np
 # Regular expressions
import re  
import string  
import unidecode

# Load text file
filename_tf = '/content/drive/My Drive/Python/IR/twitter_data.txt'
file_tf = open(filename_tf, 'rt')
text_tf = file_tf.read()
file_tf.close()

# Text file before preprocessing
print(text_tf)
print('=====================================================================================================================================\n')

# Convert to lower case
text_tf = text_tf.lower()

# remove all html tags including <br /> tag
clean = re.compile('<.*?>')
text = re.sub(clean, '', text)

# Remove all URLs
clean = re.compile(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b')
text_tf = re.sub(clean, '', text_tf)

# Strip Non Alpha Numeric characters
#text_tf = re.sub(r'\W+', ' ', text_tf)

# Remove accented string
text_tf = unidecode.unidecode(text_tf)

# remove unicode decimal digit
text_tf = re.sub('\d+', '', text_tf)

# remove leading and ending whitespace
text_tf = text_tf.strip()

# remove punctuation
text_output_tf = text_tf.translate(str.maketrans('', '', string.punctuation))

# split text by white space
text_output_tf = text_output_tf.split()

# text file after preprocessing
print(text_output_tf)

result_textfile_tf = (' '.join(str(word) for word in text_output_tf))
print(result_textfile_tf)

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))
tokens = word_tokenize(result_textfile_tf)
result_tf = [i for i in tokens if not i in stop_words]
#Array form
print (result_tf)
#String form
result_textfile_tf2 = (' '.join(str(token) for token in result_tf))
# Result text file after removing stop words
print(result_textfile_tf2)

pip install pyspellchecker

from spellchecker import SpellChecker

spell = SpellChecker()

# find those words that may be misspelled
misspelled_text_tf = spell.unknown(result_tf)
print("Mispelled count: {}".format(len(misspelled_text_tf)))
print(misspelled_text_tf)

for word in misspelled_text_tf:
    corrected_word = spell.correction(word)
    candidate_words = spell.candidates(word)
    

    # Get the one `most likely` answer
    print('word :' + ' ' + word + '\n' + 'corrected word : ' + corrected_word)
  
    # Get a list of `likely` options
    print('candidate words :')
    print(candidate_words)

pip install textblob

# Spell Correcting using TextBlob
from textblob import TextBlob

# file with incorrect spelling
f1 = result_textfile_tf2          
print("original text: \n"+str(f1))

# Corrects the spelling
corrected_file_tf = str(TextBlob(f1).correct())
 
# prints the corrected spelling
print("corrected text: \n"+ corrected_file_tf)

# Stemming using NLTK

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer= PorterStemmer()
result_stem_tf = word_tokenize(corrected_file_tf)
for word in result_stem_tf:
    print(stemmer.stem(word))

from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import word_tokenize

stemmer2= SnowballStemmer('english')
result_stem_tf2 = word_tokenize(corrected_file_tf)
for word in result_stem_tf2:
    print(stemmer2.stem(word))

# Lemmatization using NLTK

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

lemmatizer=WordNetLemmatizer()
result_lem_tf = word_tokenize(corrected_file_tf)
for word in result_lem_tf:
    print(lemmatizer.lemmatize(word))

# Lemmatization using TextBlob
from textblob import TextBlob, Word
from nltk.tokenize import word_tokenize

result_lem_tf2 = word_tokenize(corrected_file_tf)
lem2 = TextBlob(corrected_file_tf)
print([w.lemmatize() for w in lem2.words])