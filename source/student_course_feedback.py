# -*- coding: utf-8 -*-
"""student_course_feedback.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11zmLe25UUJTv6TOdrYYMveX4pht_b7Jh

**Assignment1 - Information Retrieval**

**209381P - T.P.N. Silva**

**Preprocess Student Course Feedback**
"""

pip install nltk

import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk import sent_tokenize, word_tokenize
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords

import pandas as pd
import numpy as np
 # Regular expressions
import re  
import string

# load text file
filename = '/content/drive/My Drive/Python/IR/student_course_feedback.txt'
file = open(filename, 'rt')
text = file.read()
file.close()

# text file before preprocessing
print(text)
print('=====================================================================================================================================\n')

# convert to lower case
text = text.lower()

# remove all html tags including <br /> tag
clean = re.compile('<.*?>')
text = re.sub(clean, '', text)

# remove unicode decimal digit
text = re.sub('\d+', '', text)

# remove leading and ending whitespace
text = text.strip()

# remove punctuation
text_output = text.translate(str.maketrans('', '', string.punctuation))

# split text by white space
text_output = text_output.split()

# text file after preprocessing
print(text_output)

result_textfile = (' '.join(str(word) for word in text_output))
print(result_textfile)

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))
tokens = word_tokenize(result_textfile)
result = [i for i in tokens if not i in stop_words]
#Array form
print (result)
#String form
result_textfile2 = (' '.join(str(token) for token in result))
# Result text file after removing stop words
print(result_textfile2)

pip install pyspellchecker

from spellchecker import SpellChecker

spell = SpellChecker()

# find those words that may be misspelled
misspelled_text = spell.unknown(result)
print("Mispelled count: {}".format(len(misspelled_text)))
print(misspelled_text)

for word in misspelled_text:
    corrected_word = spell.correction(word)
    candidate_words = spell.candidates(word)
    

    # Get the one `most likely` answer
    print('word :' + ' ' + word + '\n' + 'corrected word : ' + corrected_word)
  
    # Get a list of `likely` options
    print('candidate words :')
    print(candidate_words)

# Spell Correcting using TextBlob
from textblob import TextBlob

# file with incorrect spelling
f1 = result_textfile2           
print("original text: \n"+str(f1))

# Corrects the spelling
corrected_file = str(TextBlob(f1).correct())
 
# prints the corrected spelling
print("corrected text: \n"+ corrected_file)

# Stemming using NLTK

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer= PorterStemmer()
result_stem = word_tokenize(corrected_file)
for word in result_stem:
    print(stemmer.stem(word))

from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import word_tokenize

stemmer2= SnowballStemmer('english')
result_stem2 = word_tokenize(corrected_file)
for word in result_stem2:
    print(stemmer2.stem(word))

pip install textblob

# Lemmatization using NLTK

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

lemmatizer=WordNetLemmatizer()
result_lem = word_tokenize(corrected_file)
for word in result_lem:
    print(lemmatizer.lemmatize(word))

# Lemmatization using TextBlob
from textblob import TextBlob, Word
from nltk.tokenize import word_tokenize

result_lem = word_tokenize(corrected_file)
lem2 = TextBlob(corrected_file)
print([w.lemmatize() for w in lem2.words])