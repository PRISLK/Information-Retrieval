# -*- coding: utf-8 -*-
"""research_paper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_u9cTrXqhJeMfbXwZE8mY9p8DKOcGWZG

**Assignment1 - Information Retrieval**

**209381P - T.P.N. Silva**

**Preprocess Research paper**
"""

pip install nltk

pip install Unidecode

# Import libaries

import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk import sent_tokenize, word_tokenize
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords

import pandas as pd
import numpy as np
 # Regular expressions
import re  
import string  
import unidecode

# Load text file
filename_rp = '/content/drive/My Drive/Python/IR/research_paper.txt'
file_rp = open(filename_rp, 'rt')
text_rp = file_rp.read()
file_rp.close()

# Text file before preprocessing
print(text_rp)
print('=====================================================================================================================================\n')

# Convert to lower case
text_rp = text_rp.lower()

# Remove all URLs
clean = re.compile(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b')
text_rp = re.sub(clean, '', text_rp)

# Strip Non Alpha Numeric characters
#text_tp = re.sub(r'\W+', ' ', text_tp)

# Remove accented string
text_rp = unidecode.unidecode(text_rp)

# remove unicode decimal digit
text_rp = re.sub('\d+', '', text_rp)

# remove leading and ending whitespace
text_rp = text_rp.strip()

# remove punctuation
text_output_rp = text_rp.translate(str.maketrans('', '', string.punctuation))

# split text by white space
text_output_rp = text_output_rp.split()

# text file after preprocessing
print(text_output_rp)

result_textfile_rp = (' '.join(str(word) for word in text_output_rp))
print(result_textfile_rp)

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))
tokens = word_tokenize(result_textfile_rp)
result_rp = [i for i in tokens if not i in stop_words]
#Array form
print (result_rp)
#String form
result_textfile_rp2 = (' '.join(str(token) for token in result_rp))
# Result text file after removing stop words
print(result_textfile_rp2)

pip install pyspellchecker

from spellchecker import SpellChecker

spell = SpellChecker()

# find those words that may be misspelled
misspelled_text_rp = spell.unknown(result_rp)
print("Mispelled count: {}".format(len(misspelled_text_rp)))
print(misspelled_text_rp)

for word in misspelled_text_rp:
    corrected_word = spell.correction(word)
    candidate_words = spell.candidates(word)
    

    # Get the one `most likely` answer
    print('word :' + ' ' + word + '\n' + 'corrected word : ' + corrected_word)
  
    # Get a list of `likely` options
    print('candidate words :')
    print(candidate_words)

pip install textblob

# Spell Correcting using TextBlob
from textblob import TextBlob

# file with incorrect spelling
f1 = result_textfile_rp2          
print("original text: \n"+str(f1))

# Corrects the spelling
corrected_file_rp = str(TextBlob(f1).correct())
 
# prints the corrected spelling
print("corrected text: \n"+ corrected_file_rp)

# Stemming using NLTK

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer= PorterStemmer()
result_stem = word_tokenize(corrected_file_rp)
for word in result_stem:
    print(stemmer.stem(word))

from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import word_tokenize

stemmer2= SnowballStemmer('english')
result_stem2 = word_tokenize(corrected_file_rp)
for word in result_stem2:
    print(stemmer2.stem(word))

# Lemmatization using NLTK

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

lemmatizer=WordNetLemmatizer()
result_lem = word_tokenize(corrected_file_rp)
for word in result_lem:
    print(lemmatizer.lemmatize(word))

# Lemmatization using TextBlob
from textblob import TextBlob, Word
from nltk.tokenize import word_tokenize

result_lem = word_tokenize(corrected_file_rp)
lem2 = TextBlob(corrected_file_rp)
print([w.lemmatize() for w in lem2.words])